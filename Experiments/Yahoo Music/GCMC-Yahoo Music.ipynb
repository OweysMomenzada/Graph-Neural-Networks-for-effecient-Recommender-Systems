{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yahoo Music Datensatz\n",
    "\n",
    "Entnommen wurde der Datensatz aus https://webscope.sandbox.yahoo.com/catalog.php?datatype=r&did=3. \n",
    "Dieser ist der Yahoo Music Datensatz mit **min. 300.000 Bewertungen**. Dies ist ein 0-Core Datensatz. \n",
    "Daher werden User mit weniger als 10 Bewertungen (**10-Core**) gefiltert, um das Kaltstart-Problem zu vermeiden.\n",
    "\n",
    "**Datensatz-Quelle:** <br />\n",
    "Noam Koenigstein, Gideon Dror, Yehuda Koren (2011) <br />\n",
    "Yahoo! music recommendations:  <br />\n",
    "modeling music ratings with temporal dynamics and item taxonomy <br />\n",
    "DOI = https://doi.org/10.1145/2043932.2043964 <br />\n",
    "\n",
    "Die Implementation des Mini-Batche-Samplings (<i>class MinibatchSampler</i>) und des GCMC-Modells wurde (<i>class GCMCConv, class GCMCLayer, GCMCRating </i>) von der DGL-Library für das MovieLens Datenset (mit Features) zur Verfügung gestellt. In diesem Notebook wurde die Implementation so geändert, dass Vorhersagen auch ohne die nötigen Features und dem MovieLens Datensatz berechnet werden können.\n",
    "\n",
    "**DGL:**<br />\n",
    "Minjie Wang and Da Zheng and Zihao Ye and Quan Gan and Mufei Li and Xiang Song and Jinjing Zhou and Chao Ma and Lingfan Yu and Yu Gai and Tianjun Xiao and Tong He and George Karypis and Jinyang Li and Zheng Zhang (2019): <br />\n",
    "Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks <br />\n",
    "arXiv preprint arXiv = 1909.01315\n",
    "\n",
    "\n",
    "**Implementation eines Collaborativen Recommender Systems:** <br />\n",
    "Bei diesem Informationssystem handelt es sich um ein Collaborative Filtering Recommender System.\n",
    "Hier werden also keine weitere Daten außer Ratings, User-IDs und Item-IDs verwendet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "# Import des Datensatzes und der nötigen Bibliotheken\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import dgl.nn as dglnn\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Komplettes set. Errors vom fullset können ignoriert werden\n",
    "trainset  = pd.read_csv('ydata-ymusic-rating-study-v1_0-train.txt', header = None, sep = '\\t',  encoding='latin-1', error_bad_lines=False ) \n",
    "testset  = pd.read_csv('ydata-ymusic-rating-study-v1_0-test.txt', header = None, sep = '\\t',  encoding='latin-1', error_bad_lines=False ) \n",
    "\n",
    "\n",
    "# Aus Simplizität wird immer von userId, itemId und rating gesprochen\n",
    "trainset.rename({0 : 'userId', 1 : 'itemId', 2 : 'rating'}, axis = 1, inplace = True)\n",
    "testset.rename({0 : 'userId', 1 : 'itemId', 2 : 'rating'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konstruiere bipartiten Graphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellt einen Tensor aus einer Liste. Ordnet Werte der Liste\n",
    "# zu Unique-Values zu, falls bool = True. Sonst nicht.\n",
    "def buildTensor(list, bool): \n",
    "\n",
    "    if bool:\n",
    "        list = torch.LongTensor(list.astype('category')\n",
    "                                .cat.codes.values) # Konvertiere zu category damit cat.codes ausgeführt werden kann.\n",
    "    else:\n",
    "        list = torch.LongTensor(list.values)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'item': 1000, 'user': 15400},\n",
      "      num_edges={('item', 'rated-by', 'user'): 311704, ('user', 'rated', 'item'): 311704},\n",
      "      metagraph=[('item', 'user', 'rated-by'), ('user', 'item', 'rated')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-a0b820fafeb3>:3: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:143.)\n",
      "  userId = torch.LongTensor(trainset['userId'].astype('category').cat.codes.values)\n"
     ]
    }
   ],
   "source": [
    "# Bilde Tensoren User und Item für den Graphen\n",
    "# Zuordnung der Ids, da diese bei 1 Anfangen und nicht bei 0.\n",
    "userId = torch.LongTensor(trainset['userId'].astype('category').cat.codes.values) \n",
    "itemId = torch.LongTensor(trainset['itemId'].astype('category').cat.codes.values) \n",
    "\n",
    "# Bilde Tensoren User und Item zum Testen des Graphens\n",
    "userIdTest = torch.LongTensor(testset['userId'].astype('category').cat.codes.values) \n",
    "itemIdTest = torch.LongTensor(testset['itemId'].astype('category').cat.codes.values) \n",
    "\n",
    "\n",
    "# Erstelle bipartiten Graphen\n",
    "graph = dgl.heterograph({\n",
    "    ('user', 'rated', 'item'): (userId, itemId),\n",
    "    # In DGL exestieren nur gerichtete Graphen. Daher wird\n",
    "    # dies über beide Richtungen definiert\n",
    "    ('item', 'rated-by', 'user'): (itemId, userId)\n",
    "})\n",
    " \n",
    "print(graph) # Info über Graphen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Füge Ratings in die Kanten hinzu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bewertungen = buildTensor(trainset['rating'], False)\n",
    "bewertungenTest = buildTensor(testset['rating'], False) # Für spätere Testzwecke\n",
    "graph.edges['rated'].data['rating'] = bewertungen\n",
    "graph.edges['rated-by'].data['rating'] = bewertungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiere Train und Testsets als Tensor für das Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere Test und Trainset für das Modell über Tensoren\n",
    "tensorTrainset = TensorDataset(userId, itemId, bewertungen)\n",
    "tensorTestset = TensorDataset(userIdTest, itemIdTest, bewertungenTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellung von Minibatches & Neighbor Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausführung des Mini-Batchings\n",
    "class MinibatchSampler(object):\n",
    "    \n",
    "    def __init__(self, graph, num_layers):\n",
    "        self.graph = graph\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    # Finde die nötigen Knoten und konstruiere den Pair Graphen\n",
    "    def sample(self, batch):\n",
    "        # Konvertiere die Liste des Batches (Trainset) in 3 verschiedenen Vektoren.\n",
    "        users, items, ratings = zip(*batch)\n",
    "        users = torch.stack(users)\n",
    "        items = torch.stack(items)\n",
    "        ratings = torch.stack(ratings)\n",
    "        \n",
    "        # Konstruiere Bipartiten-Graph auf Grundlage des Batches.\n",
    "        pair_graph = dgl.heterograph(\n",
    "            {('user', 'rated', 'item'): (users, items)},\n",
    "            num_nodes_dict={'user': self.graph.number_of_nodes('user'), 'item': self.graph.number_of_nodes('item')})\n",
    "        \n",
    "        pair_graph = dgl.compact_graphs(pair_graph) # Löscht Knoten die für das Sampling nicht genutzt werden.\n",
    "        pair_graph.edata['rating'] = ratings # Füge die Ratings in die Sampling-Edges hinzu.\n",
    "        \n",
    "        # Konstruiere Blocks\n",
    "        seeds = {'user': pair_graph.nodes['user'].data[dgl.NID], # Bestimme Seed-nodes über NID (Note-ID)\n",
    "                 'item': pair_graph.nodes['item'].data[dgl.NID]}\n",
    "        blocks = self.construct_blocks(seeds, (users, items)) # Hier werden Blöcke für die Seed-nodes konstruiert.\n",
    "        \n",
    "        # Speichert Node-Features vom Input Graphen in die Samplings.\n",
    "        for feature_name in self.graph.nodes['user'].data.keys():\n",
    "            blocks[0].srcnodes['user'].data[feature_name] = \\\n",
    "                self.graph.nodes['user'].data[feature_name][blocks[0].srcnodes['user'].data[dgl.NID]] \n",
    "            \n",
    "        for feature_name in self.graph.nodes['item'].data.keys():\n",
    "            blocks[0].srcnodes['item'].data[feature_name] = \\\n",
    "                self.graph.nodes['item'].data[feature_name][blocks[0].srcnodes['item'].data[dgl.NID]]\n",
    "        \n",
    "        # Ergebnis ist ein Mini-Batch.\n",
    "        return pair_graph, blocks \n",
    "    \n",
    "    # Berechne den Block\n",
    "    def construct_blocks(self, seeds, user_item_pairs_to_remove): #Seed-Knoten als Ziel-Knoten definiert\n",
    "        blocks = []\n",
    "        users, items = user_item_pairs_to_remove\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # übernimmt alle Nachbarn von den Seed-Nodes\n",
    "            sampled_graph = dgl.in_subgraph(self.graph, seeds) \n",
    "            # Sampling für beide Richtungen finden.\n",
    "            sampled_eids = sampled_graph.edges['rated'].data[dgl.EID]\n",
    "            sampled_eids_rev = sampled_graph.edges['rated-by'].data[dgl.EID]\n",
    "            \n",
    "            # Hier werden die Kanten entfernt die nicht im Training-Prozess benötigt werden. \n",
    "            _, _, edges_to_remove = sampled_graph.edge_ids(users, items, etype='rated', return_uv=True)  \n",
    "            _, _, edges_to_remove_rev = sampled_graph.edge_ids(items, users, etype='rated-by', return_uv=True)\n",
    "            \n",
    "            sampled_with_edges_removed = sampled_graph\n",
    "            if len(edges_to_remove) > 0:\n",
    "                sampled_with_edges_removed = dgl.remove_edges(\n",
    "                    sampled_with_edges_removed, edges_to_remove, 'rated')\n",
    "                sampled_eids = sampled_eids[sampled_with_edges_removed.edges['rated'].data[dgl.EID]]\n",
    "                \n",
    "            if len(edges_to_remove_rev) > 0:\n",
    "                sampled_with_edges_removed = dgl.remove_edges(\n",
    "                    sampled_with_edges_removed, edges_to_remove_rev, 'rated-by')\n",
    "                sampled_eids_rev = sampled_eids_rev[sampled_with_edges_removed.edges['rated-by'].data[dgl.EID]]\n",
    "            \n",
    "            # Konstruiere einen Block vom gesampelten Graphen\n",
    "            block = dgl.to_block(sampled_with_edges_removed, seeds)\n",
    "            blocks.insert(0, block)\n",
    "            seeds = {'user': block.srcnodes['user'].data[dgl.NID],\n",
    "                     'item': block.srcnodes['item'].data[dgl.NID]}\n",
    "            \n",
    "            # Kopiere die Bewertungen zu den Kanten der gesampelten Graphen\n",
    "            block.edges['rated'].data['rating'] = \\\n",
    "                self.graph.edges['rated'].data['rating'][sampled_eids]\n",
    "            block.edges['rated-by'].data['rating'] = \\\n",
    "                self.graph.edges['rated-by'].data['rating'][sampled_eids_rev]\n",
    "            \n",
    "        return blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiere das GCMC-Modell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class GCMCConv(nn.Module): # Die Funktionen dieser Klasse beschreibt einen Encoder\n",
    "\n",
    "    def __init__(self, hidden_dims, num_ratings):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Die Bewertungen sind von 1 bis 5 nummeriert, daher muss eine 1 addiert werden. \n",
    "        self.W_r = nn.Parameter(torch.randn(num_ratings + 1, hidden_dims, hidden_dims)) # Initialisierung der erlernbaren Gewichtsmatrix in der Messagefunktion\n",
    "        self.W_i = nn.Linear(hidden_dims * 2, hidden_dims) # Die Gewichtsmatrix W_i in der Akkumulationsfunktion\n",
    "        \n",
    "    def compute_message(self, W, edges): \n",
    "        W_r = W[edges.data['rating']] # W_r als Edge Feature für das Rating r. \n",
    "        h = edges.src['h'] # h als die h-te Zwischenebene des Encoders im GCMC, sowohl für User und Item\n",
    "        m = (W_r @ h.unsqueeze(-1)).squeeze(2) # m als die Berechnete Message μ\n",
    "        return m\n",
    "\n",
    "    def forward(self, graph, node_features):\n",
    "        with graph.local_scope():\n",
    "            src_features, dst_features = node_features\n",
    "            \n",
    "            # Anwendung von compute_message auf alle Edgefeatures des Inputs\n",
    "            graph.srcdata['h'] = src_features \n",
    "            graph.dstdata['h'] = dst_features \n",
    "            \n",
    "             # Die Aggregation, wobei dies über den Schnitt der Nachbarschaft erfolgt\n",
    "            graph.apply_edges(lambda edges: {'m': self.compute_message(self.W_r, edges)})\n",
    "            \n",
    "            # Updates der Repräsentationen von Output Users und Items\n",
    "            graph.update_all(fn.copy_e('m', 'm'), fn.mean('m', 'h_neigh'))  \n",
    "            \n",
    "            # Akkumulationsfunktion mit Konkatenation\n",
    "            result = F.relu(self.W_i(torch.cat([graph.dstdata['h'], graph.dstdata['h_neigh']], 1))) \n",
    "            return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCMCLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dims, num_ratings):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Hier werden die Ebenen h_ui und h_vj für beide Richtungen ausgerechnet.\n",
    "        self.heteroconv = dglnn.HeteroGraphConv(\n",
    "            {'rated': GCMCConv(hidden_dims, num_ratings), 'rated-by': GCMCConv(hidden_dims, num_ratings)},\n",
    "            aggregate='sum')\n",
    "                \n",
    "    def forward(self, block, input_user_features, input_item_features):\n",
    "        with block.local_scope():\n",
    "            # Input-Vektoren für die h.te Ebene\n",
    "            h_user = input_user_features \n",
    "            h_item = input_item_features\n",
    "            \n",
    "            # übernehme Features von vorherhiger Ebene für nächste Ebene\n",
    "            src_features = {'user': h_user, 'item': h_item} \n",
    "            dst_features = {'user': h_user[:block.number_of_dst_nodes('user')],\n",
    "                            # Analog, jedoch mit Beachtung der Samplings\n",
    "                            'item': h_item[:block.number_of_dst_nodes('item')]} \n",
    "            \n",
    "            result = self.heteroconv(block, (src_features, dst_features))\n",
    "            return result['user'], result['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCMCRating(nn.Module):\n",
    "    def __init__(self, num_users, num_items, hidden_dims, num_ratings, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddingvektor der Größe hidden_dims für User und Items\n",
    "        self.user_embeddings = nn.Embedding(num_users, hidden_dims)  \n",
    "        self.item_embeddings = nn.Embedding(num_items, hidden_dims)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            # Berechnung der Layers im Encoder.\n",
    "            GCMCLayer(hidden_dims, num_ratings) for _ in range(num_layers)]) \n",
    "        \n",
    "        # Trainierbarer Parameter für Items W_v und für Users W_u\n",
    "        self.W_u = nn.Linear(hidden_dims, hidden_dims) # Linear() besteht aus x*W_u^T + b wobei b ein Bias ist.\n",
    "        self.W_v = nn.Linear(hidden_dims, hidden_dims)\n",
    "        \n",
    "    def forward(self, blocks):\n",
    "        # Beginne mit Embedding für jeden User und Item\n",
    "        user_embeddings = self.user_embeddings(blocks[0].srcnodes['user'].data[dgl.NID])\n",
    "        item_embeddings = self.item_embeddings(blocks[0].srcnodes['item'].data[dgl.NID])\n",
    "        \n",
    "        # Iteriere über die Encoder-Layers\n",
    "        for block, layer in zip(blocks, self.layers):\n",
    "            # Berechnung der Nachricht zwischen Item und User\n",
    "            user_embeddings, item_embeddings = layer(block, user_embeddings, item_embeddings) \n",
    "        \n",
    "        # Zusammensetzung des Embeddingvektors und dazugehörigen trainierbaren Gewichtung \n",
    "        z_u = self.W_u(user_embeddings) \n",
    "        z_v = self.W_v(item_embeddings)\n",
    "        \n",
    "        return z_u, z_v # Finale Repräsentation der Knoten als Embeddingvektoren z_u und z_v\n",
    "        \n",
    "        # Decoder über das Skalarprodukt  \n",
    "    def compute_score(self, pair_graph, z_u, z_v):\n",
    "        with pair_graph.local_scope():\n",
    "            # Nutze für die Ebene h die Embeddings z_u und z_v\n",
    "            pair_graph.nodes['user'].data['h'] = z_u \n",
    "            pair_graph.nodes['item'].data['h'] = z_v\n",
    "            \n",
    "            # Berechne Rating über Skalarpodukt über z_u und z_v  und update die Kantenfeatures\n",
    "            pair_graph.apply_edges(fn.u_dot_v('h', 'h', 'r')) \n",
    "            \n",
    "            return pair_graph.edata['r'] #Ende des Forward-Propagation vom GCMC-Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE zur Berechnung der Kostenfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training des GCMC-Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def trainingLoop(NUM_LAYERS, BATCH_SIZE, NUM_EPOCHS, HIDDEN_DIMS, NUM_RATINGS, printing = True):\n",
    "    sampler = MinibatchSampler(graph, NUM_LAYERS) # Erstellt ein Sampler Objekt basierend auf den Graphen\n",
    "    \n",
    "    # Sampelt und erstellt Trainset und Testset auf Basis des Samplers und Batchsize\n",
    "    train_dataloader = DataLoader(tensorTrainset, batch_size=BATCH_SIZE, collate_fn=sampler.sample, shuffle=True)\n",
    "    test_dataloader = DataLoader(tensorTestset, batch_size=BATCH_SIZE, collate_fn=sampler.sample, shuffle=False)\n",
    "        \n",
    "    # Übergabe der Hyper-Parameter und Konstruktion des Modells für das Datenset\n",
    "    model = GCMCRating(graph.number_of_nodes('user'), graph.number_of_nodes('item'), HIDDEN_DIMS, NUM_RATINGS, NUM_LAYERS) \n",
    "    \n",
    "    # SGD-Optimierungsverfahren für die Modell-Parameter mit Lernparameter = 0.01\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "    \n",
    "    rmse = []\n",
    "    \n",
    "    for i in range(NUM_EPOCHS):\n",
    "        \n",
    "        model.train() # Modell wird nun in Trainzustand gesetzt.\n",
    "       \n",
    "        with tqdm.tqdm(train_dataloader) as t: # Training über Trainset\n",
    "            for pair_graph, blocks in t:\n",
    "                user_emb, item_emb = model(blocks)\n",
    "                prediction = model.compute_score(pair_graph, user_emb, item_emb)\n",
    "                loss = ((prediction - pair_graph.edata['rating']) ** 2).mean()\n",
    "                opt.zero_grad() # setze Gradienten auf 0\n",
    "                loss.backward() # Berechne Gradienten mittels Backpropagation\n",
    "                opt.step() # update die Modell-Parameter\n",
    "\n",
    "        model.eval() # Modell wird nun in Testzustand gesetzt.\n",
    "    \n",
    "        with tqdm.tqdm(test_dataloader) as t: # Evaluation über Testset\n",
    "            with torch.no_grad():\n",
    "                predictions = []\n",
    "                ratings = []\n",
    "                for pair_graph, blocks in t:\n",
    "                    # Definiere die  Embeddingvektoren von User und Item\n",
    "                    user_emb, item_emb = model(blocks) \n",
    "                    # Berechnung der Vorhersage eines Ratings r\n",
    "                    prediction = model.compute_score(pair_graph, user_emb, item_emb) \n",
    "                    predictions.append(prediction) # vorhergesagter wert von r\n",
    "                    ratings.append(pair_graph.edata['rating']) # tatsächlicher Wert von r\n",
    "\n",
    "                predictions = torch.cat(predictions, 0)\n",
    "                ratings = torch.cat(ratings, 0)\n",
    "        \n",
    "        # Ausgabe des RMSE nach jedem SGD-Schritt\n",
    "        if printing:\n",
    "            print('RMSE:', mean_squared_error(predictions, ratings, squared=True).item() , ' - Nach',i+1,'. Epoch:')\n",
    "        \n",
    "        rmse.append(mean_squared_error(predictions, ratings, squared=True).item())\n",
    "    \n",
    "    # Gibt den endgültigen RMSE aus, falls printing = True\n",
    "    if printing:\n",
    "        print('\\n\\nAuswertung für folgende Hyper-Parameter: \\n',\n",
    "              'NUM_LAYERS','=', NUM_LAYERS, '\\n',\n",
    "              'BATCH_SIZE','=', BATCH_SIZE, '\\n',\n",
    "              'NUM_EPOCHS','=', NUM_EPOCHS, '\\n',\n",
    "              'HIDDEN_DIMS','=', HIDDEN_DIMS, '\\n') \n",
    "        print('Endgültiger RMSE:', mean_squared_error(predictions, ratings, squared=True).item())\n",
    "    \n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel Hyper-Parameter für das Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 312/312 [01:49<00:00,  2.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 54/54 [00:05<00:00,  9.46it/s]\n",
      "  0%|                                                                                          | 0/312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.258394111961539  - Nach 1 . Epoch:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 312/312 [01:50<00:00,  2.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 54/54 [00:05<00:00,  9.37it/s]\n",
      "  0%|                                                                                          | 0/312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.353123223166078  - Nach 2 . Epoch:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████▌                                                 | 119/312 [00:41<01:07,  2.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a21607657170>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mNUM_RATINGS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Anzahl der Bewertungselemente der Bewertungsmenge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainingLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_LAYERS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHIDDEN_DIMS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_RATINGS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-e19c11f147a5>\u001b[0m in \u001b[0;36mtrainingLoop\u001b[1;34m(NUM_LAYERS, BATCH_SIZE, NUM_EPOCHS, HIDDEN_DIMS, NUM_RATINGS, printing)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Training über Trainset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mpair_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                 \u001b[0muser_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1165\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1166\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-d95aee860cf9>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     25\u001b[0m         seeds = {'user': pair_graph.nodes['user'].data[dgl.NID], # Bestimme Seed-nodes über NID (Note-ID)\n\u001b[0;32m     26\u001b[0m                  'item': pair_graph.nodes['item'].data[dgl.NID]}\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Hier werden Blöcke für die Seed-nodes konstruiert.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Speichert Node-Features vom Input Graphen in die Samplings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d95aee860cf9>\u001b[0m in \u001b[0;36mconstruct_blocks\u001b[1;34m(self, seeds, user_item_pairs_to_remove)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# Konstruiere einen Block vom gesampelten Graphen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_with_edges_removed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             seeds = {'user': block.srcnodes['user'].data[dgl.NID],\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dgl\\transform.py\u001b[0m in \u001b[0;36mto_block\u001b[1;34m(g, dst_nodes, include_dst_in_src)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[0mdst_node_ids_nd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dgl_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnodes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdst_node_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1722\u001b[1;33m     new_graph_index, src_nodes_nd, induced_edges_nd = _CAPI_DGLToBlock(\n\u001b[0m\u001b[0;32m   1723\u001b[0m         g._graph, dst_node_ids_nd, include_dst_in_src)\n\u001b[0;32m   1724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dgl\\_ffi\\_ctypes\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mret_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDGLValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mret_tcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         check_call(_LIB.DGLFuncCall(\n\u001b[0m\u001b[0;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m             ctypes.byref(ret_val), ctypes.byref(ret_tcode)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyper-Parameter des GCMC-Modells\n",
    "NUM_LAYERS = 1 # Ebenen des Encoders\n",
    "BATCH_SIZE = 1000 # Batch-Siz e für das Sampling\n",
    "NUM_EPOCHS = 15 # Anzahl der SGD Iterationen\n",
    "HIDDEN_DIMS = 8 # Länge des Vektors für einen Knoten\n",
    "NUM_RATINGS = len(set(trainset['rating'])) # Anzahl der Bewertungselemente der Bewertungsmenge\n",
    "\n",
    "rmse = trainingLoop(NUM_LAYERS, BATCH_SIZE, NUM_EPOCHS, HIDDEN_DIMS, NUM_RATINGS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
